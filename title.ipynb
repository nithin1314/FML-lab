{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAR+SYzuZpWiUnWsI0h34P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithin1314/FML-lab/blob/main/title.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31KPK9ENCvnp",
        "outputId": "7c4bea99-75be-4e0b-baed-5dc345c98eb9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PcC7YyeCbYR",
        "outputId": "65768703-5b90-4bc5-e3eb-2ae3a08ab4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['We', 'are', 'learning', 'Natural', 'Language', 'Processing', 'as', 'part', 'of', 'Fundamentals', 'of', 'Machine', 'Learning', 'in', 'our', 'second', 'year', 'B.Tech', '.']\n",
            "['We', 'learning', 'Natural', 'Language', 'Processing', 'part', 'Fundamentals', 'Machine', 'Learning', 'second', 'year', 'B.Tech', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"We are learning Natural Language Processing as part of\n",
        "                Fundamentals of Machine Learning in our second year B.Tech.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jOI5YaegEu24",
        "outputId": "1cef62ce-2528-4f7a-8df4-a50377059ac8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = \"\"\"Holy cow! screamed Jane. My mother-in-law's rants make me furious!\"\"\"\n",
        "\n",
        "pwords = set(string.punctuation)\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "#filtered_sentence = [w for w in word_tokens if not w.lower() in pwords]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in pwords:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yVeI7B9EyeY",
        "outputId": "cc4ab36d-8499-4a30-e9ca-2c6353e49187"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Holy', 'cow', '!', 'screamed', 'Jane', '.', 'My', 'mother-in-law', \"'s\", 'rants', 'make', 'me', 'furious', '!']\n",
            "['Holy', 'cow', 'screamed', 'Jane', 'My', 'mother-in-law', \"'s\", 'rants', 'make', 'me', 'furious']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = \"\"\"Jane and Jack went to the market. Her son, John Jones Junior. was born on Decemver. 6, 2008.\"\"\"\n",
        "\n",
        "pwords = set(string.punctuation)\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "#filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in pwords:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZlqb4agFibk",
        "outputId": "57dbd1de-c416-4989-e26b-299adebd904e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jane', 'and', 'Jack', 'went', 'to', 'the', 'market', '.', 'Her', 'son', ',', 'John', 'Jones', 'Junior', '.', 'was', 'born', 'on', 'Decemver', '.', '6', ',', '2008', '.']\n",
            "['Jane', 'and', 'Jack', 'went', 'to', 'the', 'market', 'Her', 'son', 'John', 'Jones', 'Junior', 'was', 'born', 'on', 'Decemver', '6', '2008']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TOKENIZATION***"
      ],
      "metadata": {
        "id": "esjDDPIzNuud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t=\"\"\"Naruto was serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump from September 1999 to November 2014, with its chapters collected in 72 tankōbon volumes. Part I of the manga was adapted into an anime television series by Pierrot and Aniplex, which ran for 220 episodes from October 2002 to February 2007 on TV Tokyo; the English dub of the series aired on Cartoon Network and YTV from September 2005 to December 2009. \"\"\"\n",
        "to=t.split()\n",
        "print(to)\n",
        "print(\"no.of :\",len(to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vGpJa0OHBrr",
        "outputId": "8d855f65-549e-493e-9812-792495378af6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Naruto', 'was', 'serialized', 'in', \"Shueisha's\", 'shōnen', 'manga', 'magazine', 'Weekly', 'Shōnen', 'Jump', 'from', 'September', '1999', 'to', 'November', '2014,', 'with', 'its', 'chapters', 'collected', 'in', '72', 'tankōbon', 'volumes.', 'Part', 'I', 'of', 'the', 'manga', 'was', 'adapted', 'into', 'an', 'anime', 'television', 'series', 'by', 'Pierrot', 'and', 'Aniplex,', 'which', 'ran', 'for', '220', 'episodes', 'from', 'October', '2002', 'to', 'February', '2007', 'on', 'TV', 'Tokyo;', 'the', 'English', 'dub', 'of', 'the', 'series', 'aired', 'on', 'Cartoon', 'Network', 'and', 'YTV', 'from', 'September', '2005', 'to', 'December', '2009.']\n",
            "no.of : 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=\"\"\"Naruto was serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump from September 1999 to November 2014, with its chapters collected in 72 tankōbon volumes. Part I of the manga was adapted into an anime television series by Pierrot and Aniplex, which ran for 220 episodes from October 2002 to February 2007 on TV Tokyo; the English dub of the series aired on Cartoon Network and YTV from September 2005 to December 2009. \"\"\"\n",
        "to=t.split(',')\n",
        "print(to)\n",
        "print(\"no.of :\",len(to))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ll1QvvaOKVr",
        "outputId": "493cec65-a477-4bbe-86ab-fcb6e721bda4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Naruto was serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump from September 1999 to November 2014\", ' with its chapters collected in 72 tankōbon volumes. Part I of the manga was adapted into an anime television series by Pierrot and Aniplex', ' which ran for 220 episodes from October 2002 to February 2007 on TV Tokyo; the English dub of the series aired on Cartoon Network and YTV from September 2005 to December 2009. ']\n",
            "no.of : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***using Regular expressioin***"
      ],
      "metadata": {
        "id": "3zTaSgBHQeLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "atRTjB_IOqMo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t='''Naruto was serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump from September 1999 to November 2014, with its chapters collected in 72 tankōbon volumes Part I of the manga was adapted into an anime television series by Pierrot and Aniplex, which ran for 220 episodes from October 2002 to February 2007 on TV Tokyo, the English dub of the series aired on Cartoon Network and YTV from September 2005 to December 2009 '''"
      ],
      "metadata": {
        "id": "TnEsluCyR3hU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "too=re.findall(\"[\\w']+\",t)\n",
        "print(too)\n",
        "print(\"no\",len(too))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaHEHyNIQpMd",
        "outputId": "36d66c96-e53d-4648-ccd3-c4ab82ea9f4a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Naruto', 'was', 'serialized', 'in', \"Shueisha's\", 'shōnen', 'manga', 'magazine', 'Weekly', 'Shōnen', 'Jump', 'from', 'September', '1999', 'to', 'November', '2014', 'with', 'its', 'chapters', 'collected', 'in', '72', 'tankōbon', 'volumes', 'Part', 'I', 'of', 'the', 'manga', 'was', 'adapted', 'into', 'an', 'anime', 'television', 'series', 'by', 'Pierrot', 'and', 'Aniplex', 'which', 'ran', 'for', '220', 'episodes', 'from', 'October', '2002', 'to', 'February', '2007', 'on', 'TV', 'Tokyo', 'the', 'English', 'dub', 'of', 'the', 'series', 'aired', 'on', 'Cartoon', 'Network', 'and', 'YTV', 'from', 'September', '2005', 'to', 'December', '2009']\n",
            "no 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen = re.compile('[.?!] ').split(t)\n",
        "print(sen)\n",
        "print(\"No.of sentences : \", len(sen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiJ9bKF7RQe8",
        "outputId": "7c7650ac-b72c-4dfb-ad31-b3de24d1d6b6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Naruto was serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump from September 1999 to November 2014, with its chapters collected in 72 tankōbon volumes Part I of the manga was adapted into an anime television series by Pierrot and Aniplex, which ran for 220 episodes from October 2002 to February 2007 on TV Tokyo, the English dub of the series aired on Cartoon Network and YTV from September 2005 to December 2009 \"]\n",
            "No.of sentences :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***using NKLT***"
      ],
      "metadata": {
        "id": "aUs3ZHjiYRKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "ArSq9j69UjAr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"very\"))\n",
        "print(porter.stem(\"troubleing\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad8U6wYQYc_t",
        "outputId": "6f9606a7-234b-4a7c-a953-4d3c4557a456"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "veri\n",
            "troubl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    print(token_words)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"!\".join(stem_sentence)\n",
        "\n",
        "x=stemSentence(sentence)\n",
        "print(\"Sentence after stemming :\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkBM6vslYiYF",
        "outputId": "e3ea8dc5-dd2f-42be-b2ad-1ea0c9c06155"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "Sentence after stemming : python! !are! !veri! !intellig! !and! !work! !veri! !pythonli! !and! !now! !they! !are! !python! !their! !way! !to! !success! !.! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "token_words = nltk.word_tokenize(sentence)\n",
        "print(token_words)\n",
        "\n",
        "lemma_sentence=[]\n",
        "for word in token_words:\n",
        "  lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
        "  lemma_sentence.append(\" \")\n",
        "\n",
        "print(\"lemmas of tokens: \", ''.join(lemma_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KukFUJyIYsM9",
        "outputId": "58ace32e-1a97-494b-9099-c01b821d842c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
            "lemmas of tokens:  He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCQAlxA1cehC",
        "outputId": "c0e21d38-f161-416d-e69f-69d42731c8b7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mS2FJIeCciiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}